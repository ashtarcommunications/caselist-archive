[[toc]]

== == 
=Existential Risk= 
==General== 
===Framing=== 
Eliminating existential risk is impossible- you have to prove intervening actors don’t solve or a specific risk outweighs the DA to win
Nick Bostrom 2011, Professor in the Faculty of Philosophy & Oxford Martin School, Director of the Future of Humanity Institute, and Director of the Programme on the Impacts of Future Technology at the University of Oxford, recipient of the 2009 Eugene R. Gannon Award for the Continued Pursuit of Human Advancement, holds a Ph.D. in Philosophy from the London School of Economics, 2011, “The Concept of Existential Risk,” Draft of a Paper published on ExistentialRisk.com, http://www.existentialrisk.com/concept.html, KHaze

These considerations suggest… tempting, is implausible.


===Short Term 1st=== 
Nuclear war comes before existential colonization risks
Lynda Williams, Faculty in Engineering/Physics at Santa Rosa Junior College, 2010 (“Irrational Dreams of Space Colonization,” Peace Review: A Journal of Social Justice, Volume 22, Issue 1, Available Online to Subscribing Institutions via Taylor & Francis Online, p. 4-5)

According to scientific theory,… create these threats on Earth?

Eliminating existential risk is impossible- maximizing short term protection from catastrophe opens the potential for future protection and is better than vague attempts to defend against any chance of extinction
Bostrom, 2002, Nick, Professor in the Faculty of Philosophy & Oxford Martin School, Director of the Future of Humanity Institute, and Director of the Programme on the Impacts of Future Technology at the University of Oxford, recipient of the 2009 Eugene R. Gannon Award for the Continued Pursuit of Human Advancement, holds a Ph.D. in Philosophy from the London School of Economics, 2011, “Analyzing Human Extinction Scenarios and Related Hazards,” http://www.nickbostrom.com/existential/risks.html, KHaze

Previous sections have argued… play our cards right.


===Squo Solves=== 
Tech innovation solves colonization in the long term- even if we don’t die you make us cavemen and eliminate the possibility of survival
Bostrom, 2009 — Nick Bostrom, Professor in the Faculty of Philosophy & Oxford Martin School, Director of the Future of Humanity Institute, and Director of the Programme on the Impacts of Future Technology at the University of Oxford, recipient of the 2009 Eugene R. Gannon Award for the Continued Pursuit of Human Advancement, holds a Ph.D. in Philosophy from the London School of Economics, 2009, “The Future of Humanity,” Geopolitics, History and International Relations, Volume 9, Issue 2, Available Online to Subscribing Institutions via ProQuest Research Library, http://www.nickbostrom.com/papers/future.pdf, KHaze

Technological innovation is the main… as Robert Heilbroner explains:


===Nuclear War 1st=== 
Nuclear arms race reduction is the ultimate existential risk reducer- any other threat can be countered later or by other actors
Bostrom, 2002, Nick, Professor in the Faculty of Philosophy & Oxford Martin School, Director of the Future of Humanity Institute, and Director of the Programme on the Impacts of Future Technology at the University of Oxford, recipient of the 2009 Eugene R. Gannon Award for the Continued Pursuit of Human Advancement, holds a Ph.D. in Philosophy from the London School of Economics, 2011, “Analyzing Human Extinction Scenarios and Related Hazards,” http://www.nickbostrom.com/existential/risks.html, KHaze

Some of the lesser existential risks… beyond the scope of this paper.

Nuclear war is an existential risk- the chance of extinction is sufficient to justify the label
Bostrom, 2002, Nick, Professor in the Faculty of Philosophy & Oxford Martin School, Director of the Future of Humanity Institute, and Director of the Programme on the Impacts of Future Technology at the University of Oxford, recipient of the 2009 Eugene R. Gannon Award for the Continued Pursuit of Human Advancement, holds a Ph.D. in Philosophy from the London School of Economics, 2011, “Analyzing Human Extinction Scenarios and Related Hazards,” http://www.nickbostrom.com/existential/risks.html, KHaze

4.2 Nuclear holocaust The US… other animal species.


====AT: Selection Bias==== 
Your impacts are too long term and improbable- nuclear war comes first- this assumes selection bias
Bostrom, 2005, Nick, Professor in the Faculty of Philosophy & Oxford Martin School, Director of the Future of Humanity Institute, and Director of the Programme on the Impacts of Future Technology at the University of Oxford, recipient of the 2009 Eugene R. Gannon Award for the Continued Pursuit of Human Advancement, holds a Ph.D. in Philosophy from the London School of Economics, 2011, “How Unlikely is a Doomsday Catastrophe?” http://arxiv.org/pdf/astro-ph/0512204v2.pdf, KHaze

Suppose planets get randomly… which is reassuringly small.



===Aliens=== 
====Waaaay Too Low==== 
Alien invasion is too low probability to matter even for Bostrom
Bostrom, your author, 2002, Nick, Professor in the Faculty of Philosophy & Oxford Martin School, Director of the Future of Humanity Institute, and Director of the Programme on the Impacts of Future Technology at the University of Oxford, recipient of the 2009 Eugene R. Gannon Award for the Continued Pursuit of Human Advancement, holds a Ph.D. in Philosophy from the London School of Economics, 2011, “Analyzing Human Extinction Scenarios and Related Hazards,” http://www.nickbostrom.com/existential/risks.html, KHaze

7.2 Killed by an extraterrestrial civilization… rather than a bang.

===Asteroids=== 
====Low Risk Bad==== 
<span class="Underline">Ignore low-probability impacts like short term asteroids- key to decisionmaking</span>
<span class="Underline">BENNETT 2010 (James, Prof of Economics at George Mason, The Doomsday Lobby: Hype and Panic from Sputniks, Martians, and Marauding Meteors, p. 175)</span>

<span class="Underline">Now, it makes sense… the harm will occur.”</span>


====<span class="Underline">Nuclear War Outweighs</span>==== 
<span class="Underline">Nuclear war outweighs asteroids- probability and magnitude is on our side</span>
<span class="Underline">BENNETT 2010 (James, Prof of Economics at George Mason, The Doomsday Lobby: Hype and Panic from Sputniks, Martians, and Marauding Meteors, p. 155)</span>

<span class="Underline">Given that there “is no… spent addressing those matters?</span>


===Colonization=== 
====Short Term First==== 
<span class="Underline">Short-term existential risks outweigh and turn the aff – they destroy our ability to colonize space</span>
<span class="Underline">Baum 10 – visiting scholar at Columbia University's Center for Research on Environmental Decisions, PhD candidate in Geography, and focuses on risk analysis (2/12/2010, Seth, “Is Humanity Doomed? Insights from Astrobiology”, Sustainability Journal, http://ideas.repec.org/a/gam/jsusta/v2y2010i2p591-603d7141.html) MGM</span>

<span class="Underline">The fact that… eventual space colonization.</span>


====Life Key==== 
<span class="Underline">Life is a prerequisite to space colonization</span>
<span class="Underline">Bostrom 5 [Nick – faculty of Philosophy @ Oxford Univ, “Transhumanist Values,” pdf, ZR]</span>

<span class="Underline">Basic conditions for realizing… of the transhumanist project.</span>


=Util Good= 
==Morality== 
===1NC=== 
Util key to morality- absolutist ethics risk the death of millions for one selfish person
Nye, 86 (Joseph S. 1986; Phd Political Science Harvard. University; Served as Assistant Secretary of Defense for International Security Affairs; “Nuclear Ethics” pg. 18-19)

The significance and the… age than ever before.



=Yes War= 
==War Theory Bad== 
===1NC=== 
<span class="Underline">Obsolescence theory increases the probability of war—causes states to neglect security interests</span>
<span class="Underline">Doran, Professor of International Relations at Johns Hopkins University's School of Advanced International Studies, Summer 1999 (Charles F., Survival, “Is Major War Obsolete? An Exchange: The Structural Turbulence of International Affairs,” June, vol.41 no.2, p.139-142)</span>

<span class="Underline">The conclusion, then, …end to war.</span>


==Yes Great Power War== 
===1NC=== 
<span class="Underline">Great power war possible- assumes two strong nations</span>
<span class="Underline">Mearsheimer, Professor of Political Science at the University of Chicago, February 1999 (John J., “Transcript: Is Major War Obsolete? Great Debate Series between Professor Michael Mandelbaum and Professor John J. Mearsheimer, Presider: Mr. Fareed Zakaria, http://http://www.ciaonet.org/conf/cfr10/conf/cfr10)</span>

<span class="Underline">So what I’m saying… other great powers.</span>


==AT: Deterrence== 
===1NC=== 
<span class="Underline">Deterrence is impossible</span>
<span class="Underline">Clark, associate professor of political science and director of the national security studies program at California State University, 1997 (Mark T., “Deterrence in the Second Nuclear Age-book reviews: Neorealism versus Organizational Theory,” http://www.findarticles.com/p/articles/mi_m0365/is_n1_v41/ai_19238111)</span>

<span class="Underline">Sagan's critique is a… developing its nuclear program.</span>


==<span class="Underline">AT: Interdependence </span>== 
===<span class="Underline">1NC</span>=== 
Interdependence doesn’t solve war
Kagan, Professor of History and Classics at Yale University, Summer 1999 (Donald, Survival, “Is Major War Obsolete? An Exchange: History is Full of Surprises,” June, vol.41 no.2, p.139-142)

I agree that the… the First World War.